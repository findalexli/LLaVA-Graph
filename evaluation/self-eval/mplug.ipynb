{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "first_100 = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation[:100]')\n",
    "\n",
    "# vali_dataset = load_dataset('alexshengzhili/SciCapInstructed-350K', split='1_percent_as_validation[:100]')\n",
    "# data = first_100.filter(lambda x: x['q_a_pairs'] is not None and len(x['q_a_pairs']) > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a human/AI template to organize the context as a multi-turn conversation.\n",
    "# <image> denotes an image placehold.\n",
    "\n",
    "\n",
    "def get_input(example):\n",
    "    question = example['q_a_pairs'][0][0]\n",
    "\n",
    "    prompts = [\n",
    "    f'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "    Human: <image>\n",
    "    Human: {question}.\n",
    "    AI: ''']\n",
    "    image_root_folder = '/home/ubuntu/imgs/train/'\n",
    "    image_filepath = example['image_file']\n",
    "    return prompts, [image_root_folder + image_filepath,]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "7.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "7.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "5.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "10\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "7.5\n",
      "8.5\n",
      "9.5\n",
      "7.5\n",
      "9.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "8.5\n",
      "3.5\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "10\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "10\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "10\n",
      "8.5\n",
      "7.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "8.0\n",
      "8.5\n",
      "8.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "7.5\n",
      "9.5\n",
      "9.5\n",
      "9.5\n",
      "8.5\n",
      "0.0\n",
      "9.5\n",
      "5.0\n",
      "10\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "8.5\n",
      "10\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "9.5\n",
      "8.5\n",
      "8.5\n",
      "9.5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import tqdm\n",
    "import time\n",
    "openai.api_key = \"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\"\n",
    "system_message = \"\"\"\n",
    "You are a helpful and precise assistant for checking the quality of the answer.\n",
    "You are given the graph's caption, the context of the graph, the abstract, tthe title\n",
    "\n",
    "And then you are given the question,  the answer generated by the model. Please\n",
    "think about how helpful the model answer is to the user and rate the model answer on a scale of 0 to 10, \n",
    "where 0 is not helpful at all and 10 is very helpful. Just return the floating number between 0 and 10.\n",
    "\"\"\"\n",
    "\n",
    "def construct_input_string(index):\n",
    "    content = dict()\n",
    "    cur_example = first_100[index]\n",
    "    content['title'] = cur_example['title']\n",
    "    content['abstract'] = cur_example['abstract']\n",
    "    content['caption'] = cur_example['caption']\n",
    "    content['Question to the model'] = cur_example['q_a_pairs'][0][0]\n",
    "    content['Candidate model answer'] = cur_example['q_a_pairs'][0][1]\n",
    "    return json.dumps(content)\n",
    "\n",
    "\n",
    "def get_openai_response(content_string):\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4',\n",
    "                    messages=[{\n",
    "                        'role': 'system',\n",
    "                        'content': system_message\n",
    "                    }, {\n",
    "                        'role': 'user',\n",
    "                        'content': content_string\n",
    "                    }],\n",
    "                    temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n",
    "                    max_tokens=500,\n",
    "                )['choices'][0]['message']['content']\n",
    "    return openai_response\n",
    "\n",
    "openai_responses = []\n",
    "for i in range(len(first_100)):\n",
    "    content_string = construct_input_string(i)\n",
    "    openai_response = get_openai_response(content_string)\n",
    "    print(openai_response)\n",
    "    openai_responses.append(openai_response)\n",
    "    time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_responses_float = [float(str) for str in openai_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.805, 1.3302161478496644)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(openai_responses_float), np.std(openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.47746252865508"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-350K-f54c0f55bac8c124/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5a3c9d9d0b643eb1.arrow\n"
     ]
    }
   ],
   "source": [
    "rated_data = data.add_column(\"mplug_owl_answer\", response_model)\n",
    "rated_data = rated_data.add_column(\"openai_rating\", openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/alexshengzhili/SciCapInstructed-350K/blob/main/mplug_owl_answer_and_openai_rating.jsonl'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "file_path = \"mplug_owl_answer_and_openai_rating.jsonl\"\n",
    "with open(file_path, 'w') as f:\n",
    "    for example in rated_data:\n",
    "        json_str = json.dumps(example)\n",
    "        f.write(json_str + '\\n')\n",
    "\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=file_path,\n",
    "    path_in_repo=\"mplug_owl_answer_and_openai_rating.jsonl\",\n",
    "    repo_id=\"alexshengzhili/SciCapInstructed-350K\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
