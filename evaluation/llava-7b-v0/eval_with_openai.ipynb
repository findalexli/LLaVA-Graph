{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f97ab97c5880d73a.arrow\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "jsonl_file = \"first_100_response.jsonl\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "vali_dataset = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation[:100]')\n",
    "data = vali_dataset.filter(lambda x: x['q_a_pairs'] is not None and len(x['q_a_pairs']) > 0)\n",
    "\n",
    "\n",
    "answers = []\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        answers.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What is the difference between the two graphs in Figure 10?',\n",
       "  'The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.'],\n",
       " ['What are the key observations that can be made from the graphs?',\n",
       "  'The key observations that can be made from the graphs are as follows:\\n\\n* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.\\n* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.\\n* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth['q_a_pairs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 0,\n",
       " 'prompt': 'What is the difference between the two graphs in Figure 10?',\n",
       " 'text': 'In Figure 10, there are two different graphs showing educational attainment for men and women in the United States. The first graph, which is a bar graph, displays the number of men with degrees as the vertical axis and the number of degrees as the horizontal axis. The second graph, which is a line graph, shows the trend of women earning degrees over time. The two graphs present different aspects of educational attainment, providing insights into how men and women have pursued higher education in the past and how the trend has evolved over time.',\n",
       " 'answer_id': 'aauN8seAY4GK8WUG5UGuAC',\n",
       " 'model_id': 'alexshengzhili/llava-7b-v0',\n",
       " 'metadata': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "5.0\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "0.0\n",
      "0\n",
      "9.5\n",
      "0.0\n",
      "0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "5.0\n",
      "3.0\n",
      "4.5\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "4.5\n",
      "2.0\n",
      "0\n",
      "3.0\n",
      "8.5\n",
      "3.0\n",
      "6.5\n",
      "0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.5\n",
      "5.0\n",
      "0\n",
      "6.5\n",
      "3.0\n",
      "2.0\n",
      "0.0\n",
      "4.5\n",
      "0.0\n",
      "3.5\n",
      "7.5\n",
      "0.0\n",
      "0.0\n",
      "9.5\n",
      "5.5\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "4.5\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "4.5\n",
      "4.5\n",
      "4.5\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "0.0\n",
      "9.5\n",
      "4.5\n",
      "2.0\n",
      "3.0\n",
      "0\n",
      "0.0\n",
      "2.0\n",
      "0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "0\n",
      "0\n",
      "5.0\n",
      "2.0\n",
      "0\n",
      "0\n",
      "2.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "3.5\n",
      "2.0\n",
      "2.0\n",
      "7.5\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "9.5\n",
      "3.0\n",
      "6.5\n",
      "5.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "system_message = \"\"\"\n",
    "You are a helpful and precise assistant for checking the quality of the answer.\n",
    "You are given the graph's caption, the context of the graph, the abstract, tthe title\n",
    "\n",
    "And then you are given the question, the reference answer, and the answer generated by the model. Please\n",
    "think about how helpful the model answer is to the user and rate the model answer on a scale of 0 to 10, \n",
    "where 0 is not helpful at all and 10 is very helpful. Just return the floating number between 0 and 10.\n",
    "\"\"\"\n",
    "\n",
    "def construct_input_string(index):\n",
    "    content = dict()\n",
    "    cur_example = data[index]\n",
    "    content['title'] = cur_example['title']\n",
    "    content['abstract'] = cur_example['abstract']\n",
    "    content['caption'] = cur_example['caption']\n",
    "    content['Question to the model'] = cur_example['q_a_pairs'][0][0]\n",
    "    content['reference_answer'] = cur_example['q_a_pairs'][0][1]\n",
    "    content['Candidate model answer'] = answers[index]['text']\n",
    "    return json.dumps(content)\n",
    "\n",
    "\n",
    "def get_openai_response(content_string):\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4',\n",
    "                    messages=[{\n",
    "                        'role': 'system',\n",
    "                        'content': system_message\n",
    "                    }, {\n",
    "                        'role': 'user',\n",
    "                        'content': content_string\n",
    "                    }],\n",
    "                    temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n",
    "                    max_tokens=500,\n",
    "                )['choices'][0]['message']['content']\n",
    "    return openai_response\n",
    "\n",
    "openai_responses = []\n",
    "for i in range(len(data)):\n",
    "    content_string = construct_input_string(i)\n",
    "    openai_response = get_openai_response(content_string)\n",
    "    print(openai_response)\n",
    "    openai_responses.append(openai_response)\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.305, 2.6104549411932014)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "openai_responses_float = [float(str) for str in openai_responses]\n",
    "np.mean(openai_responses_float), np.std(openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_model = [item['text'] for item in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "new_data = data.add_column(\"llava_answer\", response_model)\n",
    "rated_data = new_data.add_column(\"openai_rating\", openai_responses_float)\n",
    "file_path = \"llava_answer_and_openai_rating_first_100.jsonl\"\n",
    "with open(file_path, 'w') as f:\n",
    "    for example in rated_data:\n",
    "        json_str = json.dumps(example)\n",
    "        f.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
