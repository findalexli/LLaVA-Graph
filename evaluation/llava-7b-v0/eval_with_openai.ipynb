{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jsonl_file = \"answers.jsonl\"\n",
    "ground_truth_file = \"mplug_owl_answer_and_openai_rating.jsonl\"\n",
    "answers = []\n",
    "ground_truth = []\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        answers.append(example)\n",
    "with open(ground_truth_file, 'r') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        ground_truth.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_id': 0,\n",
       "  'prompt': 'What is the difference between the two graphs in Figure 10?',\n",
       "  'text': 'Â© copyright by Derek Bruffa, all rights reserved.',\n",
       "  'answer_id': 'Hexxd67qBhmYd8Qb2Fyjyb',\n",
       "  'model_id': 'alexshengzhili/llava-7b-v0',\n",
       "  'metadata': {}},\n",
       " {'question_id': 1,\n",
       "  'prompt': 'What is the main difference between this experiment and the one described in Section 3.1?',\n",
       "  'text': 'The main difference between this experiment and the one described in Section 3.1 is the nature of the signal being analyzed. In this image, the focus is on the pressure signal ( Depicts pressure vs. time) produced by a blood pressure machine, while in Section 3.1, the emphasis was on the current ( Depicts current vs. time) produced by a device for electrical engineering experiments. The pressure signal in this image is related to a physical phenomenon, while the current signal was used to represent the flow of electric charge in the previous experiment. Both experiments demonstrate how physical quantities are represented and recorded over time, but the specific nature of the quantities being analyzed differs between the two experiments.',\n",
       "  'answer_id': 'itYS7oYYKGhktx2oUaEee9',\n",
       "  'model_id': 'alexshengzhili/llava-7b-v0',\n",
       "  'metadata': {}},\n",
       " {'question_id': 2,\n",
       "  'prompt': 'What are the main takeaways from the graph in Figure 9?',\n",
       "  'text': 'The main takeaways from the graph in Figure 9 are the two sets of data points or measurements, possibly from a scientific or research context. These data points represent a systematic study, with one set of data points possibly showing accuracy results and the other set showing proof-of-concept measurements. The two sets of data points can help analyze and evaluate the performance of a scientific or research-related concept or idea.',\n",
       "  'answer_id': '7HpyigdBwacoGqnRyosjdZ',\n",
       "  'model_id': 'alexshengzhili/llava-7b-v0',\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file': '1801.01974v2-Figure10-1.png',\n",
       " 'id': '1801.01974v2-Figure10-1',\n",
       " 'caption': 'Fig. 10. Average AUC and AUPR versus the number of synthetic ROIs generated with DSFS, 3DMM, and SHBMM according to pose and lighting effects where still-to-video FR system employs either SVM and SRC classifiers on Chokepoint (a, b) and COX-S2V (c, d) databases.',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat is the difference between the two graphs in Figure 10?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What are the key observations that can be made from the graphs?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The key observations that can be made from the graphs are as follows:'},\n",
       "  {'from': 'human',\n",
       "   'value': '* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.'},\n",
       "  {'from': 'human',\n",
       "   'value': '* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Question: What are the implications of these observations?'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Answer: The first observation suggests that the still-to-video FR systems are able to learn the variations in pose and lighting effects from the synthetic ROIs. The second observation suggests that the SRC classifier is better at learning these variations than the SVM classifier. The third observation suggests that the still-to-video FR system is more robust to pose and lighting variations on the Chokepoint database than on the COX-S2V database.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Question: What are the limitations of the study?'}],\n",
       " 'first_mention': '2) Mixed Pose and Illumination Variations: The performance of still-to-video FR systems is assessed versus the number of synthetic ROIs generated under both pose and lighting effects. Figs. 10a and 10d show average AUC and AUPR obtained by increasing the number of synthetic ROIs used to design SRC and SVM classifiers on the Chokepoint',\n",
       " 'response': 'Question: What is the difference between the two graphs in Figure 10?\\nAnswer: The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.\\n\\nQuestion: What are the key observations that can be made from the graphs?\\nAnswer: The key observations that can be made from the graphs are as follows:\\n\\n* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.\\n* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.\\n* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.\\n\\nQuestion: What are the implications of these observations?\\nAnswer: The first observation suggests that the still-to-video FR systems are able to learn the variations in pose and lighting effects from the synthetic ROIs. The second observation suggests that the SRC classifier is better at learning these variations than the SVM classifier. The third observation suggests that the still-to-video FR system is more robust to pose and lighting variations on the Chokepoint database than on the COX-S2V database.\\n\\nQuestion: What are the limitations of the study?\\nAnswer: One limitation of the study is that it only considers two databases. It would be interesting to see how the still-to-video FR systems perform on other databases with different pose and lighting variations. Another limitation of the study is that it only considers two classifiers. It would be interesting to see how the still-to-video FR systems perform with other classifiers, such as deep learning classifiers.',\n",
       " 'title': 'Domain-Specific Face Synthesis for Video Face Recognition from a Single\\n  Sample Per Person',\n",
       " 'abstract': 'The performance of still-to-video FR systems can decline significantly\\nbecause faces captured in unconstrained operational domain (OD) over multiple\\nvideo cameras have a different underlying data distribution compared to faces\\ncaptured under controlled conditions in the enrollment domain (ED) with a still\\ncamera. This is particularly true when individuals are enrolled to the system\\nusing a single reference still. To improve the robustness of these systems, it\\nis possible to augment the reference set by generating synthetic faces based on\\nthe original still. However, without knowledge of the OD, many synthetic images\\nmust be generated to account for all possible capture conditions. FR systems\\nmay, therefore, require complex implementations and yield lower accuracy when\\ntraining on many less relevant images. This paper introduces an algorithm for\\ndomain-specific face synthesis (DSFS) that exploits the representative\\nintra-class variation information available from the OD. Prior to operation, a\\ncompact set of faces from unknown persons appearing in the OD is selected\\nthrough clustering in the captured condition space. The domain-specific\\nvariations of these face images are projected onto the reference stills by\\nintegrating an image-based face relighting technique inside the 3D\\nreconstruction framework. A compact set of synthetic faces is generated that\\nresemble individuals of interest under the capture conditions relevant to the\\nOD. In a particular implementation based on sparse representation\\nclassification, the synthetic faces generated with the DSFS are employed to\\nform a cross-domain dictionary that account for structured sparsity.\\nExperimental results reveal that augmenting the reference gallery set of FR\\nsystems using the proposed DSFS approach can provide a higher level of accuracy\\ncompared to state-of-the-art approaches, with only a moderate increase in its\\ncomputational complexity.',\n",
       " 'q_a_pairs': [['What is the difference between the two graphs in Figure 10?',\n",
       "   'The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.'],\n",
       "  ['What are the key observations that can be made from the graphs?',\n",
       "   'The key observations that can be made from the graphs are as follows:'],\n",
       "  ['* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.',\n",
       "   '* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.'],\n",
       "  ['* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.',\n",
       "   'Question: What are the implications of these observations?'],\n",
       "  ['Answer: The first observation suggests that the still-to-video FR systems are able to learn the variations in pose and lighting effects from the synthetic ROIs. The second observation suggests that the SRC classifier is better at learning these variations than the SVM classifier. The third observation suggests that the still-to-video FR system is more robust to pose and lighting variations on the Chokepoint database than on the COX-S2V database.',\n",
       "   'Question: What are the limitations of the study?']],\n",
       " 'mplug_owl_answer': 'In Figure 10, there are two graphs showing different aspects of the relationship between the number of people in a city and the amount of energy consumed per person.\\n\\nGraph a depicts the average energy consumption per person in various cities around the world. The graph shows that the energy consumption is generally lower in cities like Beijing, Seoul, and New York compared to larger cities like London and Tokyo.\\n\\nGraph b shows the number of people living in various cities, where we can see that there is a strong correlation between the number of residents and the amount of energy consumption per person. As the population increases, so does the average energy consumption per person.\\n\\nThe difference between the two graphs lies in the focus and emphasis placed on different data. Graph a highlights how different cities consume different amounts of energy per person, which may be due to differences in climate, lifestyle, or infrastructure. Graph b focuses more on the relationship between the number of residents and energy consumption, showing that as more people live in a city, the average energy consumption per person also increases.',\n",
       " 'openai_rating': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import tqdm\n",
    "import time\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "system_message = \"\"\"\n",
    "You are a helpful and precise assistant for checking the quality of the answer.\n",
    "You are given the graph's caption, the context of the graph, the abstract, tthe title\n",
    "\n",
    "And then you are given the question, the reference answer, and the answer generated by the model. Please\n",
    "think about how helpful the model answer is to the user and rate the model answer on a scale of 0 to 10, \n",
    "where 0 is not helpful at all and 10 is very helpful. Just return the floating number between 0 and 10.\n",
    "\"\"\"\n",
    "\n",
    "def construct_input_string(index):\n",
    "    content = dict()\n",
    "    cur_example = data[index]\n",
    "    content['title'] = cur_example['title']\n",
    "    content['abstract'] = cur_example['abstract']\n",
    "    content['caption'] = cur_example['caption']\n",
    "    content['Question to the model'] = cur_example['q_a_pairs'][0][0]\n",
    "    content['reference_answer'] = cur_example['q_a_pairs'][0][1]\n",
    "    content['Candidate model answer'] = response_model[index]\n",
    "    return json.dumps(content)\n",
    "\n",
    "\n",
    "def get_openai_response(content_string):\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4',\n",
    "                    messages=[{\n",
    "                        'role': 'system',\n",
    "                        'content': system_message\n",
    "                    }, {\n",
    "                        'role': 'user',\n",
    "                        'content': content_string\n",
    "                    }],\n",
    "                    temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n",
    "                    max_tokens=500,\n",
    "                )['choices'][0]['message']['content']\n",
    "    return openai_response\n",
    "\n",
    "openai_responses = []\n",
    "for i in range(len(data)):\n",
    "    content_string = construct_input_string(i)\n",
    "    openai_response = get_openai_response(content_string)\n",
    "    print(openai_response)\n",
    "    openai_responses.append(openai_response)\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
