{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using pad_token, but it is not set yet.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [03:09<00:00, 63.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.norm_f.weight', 'lang_encoder.old_decoder_blocks.0.norm_1.weight', 'lang_encoder.old_decoder_blocks.0.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.0.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.0.norm_2.weight', 'lang_encoder.old_decoder_blocks.0.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.0.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_1.weight', 'lang_encoder.old_decoder_blocks.1.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.1.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_2.weight', 'lang_encoder.old_decoder_blocks.1.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.1.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_1.weight', 'lang_encoder.old_decoder_blocks.2.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.2.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_2.weight', 'lang_encoder.old_decoder_blocks.2.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.2.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_1.weight', 'lang_encoder.old_decoder_blocks.3.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.3.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_2.weight', 'lang_encoder.old_decoder_blocks.3.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.3.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_1.weight', 'lang_encoder.old_decoder_blocks.4.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.4.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_2.weight', 'lang_encoder.old_decoder_blocks.4.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.4.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_1.weight', 'lang_encoder.old_decoder_blocks.5.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.5.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_2.weight', 'lang_encoder.old_decoder_blocks.5.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.5.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_1.weight', 'lang_encoder.old_decoder_blocks.6.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.6.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_2.weight', 'lang_encoder.old_decoder_blocks.6.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.6.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_1.weight', 'lang_encoder.old_decoder_blocks.7.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.7.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_2.weight', 'lang_encoder.old_decoder_blocks.7.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.7.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_1.weight', 'lang_encoder.old_decoder_blocks.8.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.8.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_2.weight', 'lang_encoder.old_decoder_blocks.8.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.8.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_1.weight', 'lang_encoder.old_decoder_blocks.9.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.9.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_2.weight', 'lang_encoder.old_decoder_blocks.9.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.9.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_1.weight', 'lang_encoder.old_decoder_blocks.10.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.10.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_2.weight', 'lang_encoder.old_decoder_blocks.10.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.10.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_1.weight', 'lang_encoder.old_decoder_blocks.11.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.11.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_2.weight', 'lang_encoder.old_decoder_blocks.11.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.11.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_1.weight', 'lang_encoder.old_decoder_blocks.12.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.12.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_2.weight', 'lang_encoder.old_decoder_blocks.12.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.12.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_1.weight', 'lang_encoder.old_decoder_blocks.13.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.13.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_2.weight', 'lang_encoder.old_decoder_blocks.13.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.13.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_1.weight', 'lang_encoder.old_decoder_blocks.14.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.14.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_2.weight', 'lang_encoder.old_decoder_blocks.14.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.14.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_1.weight', 'lang_encoder.old_decoder_blocks.15.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.15.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_2.weight', 'lang_encoder.old_decoder_blocks.15.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.15.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_1.weight', 'lang_encoder.old_decoder_blocks.16.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.16.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_2.weight', 'lang_encoder.old_decoder_blocks.16.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.16.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_1.weight', 'lang_encoder.old_decoder_blocks.17.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.17.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_2.weight', 'lang_encoder.old_decoder_blocks.17.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.17.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_1.weight', 'lang_encoder.old_decoder_blocks.18.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.18.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_2.weight', 'lang_encoder.old_decoder_blocks.18.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.18.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_1.weight', 'lang_encoder.old_decoder_blocks.19.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.19.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_2.weight', 'lang_encoder.old_decoder_blocks.19.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.19.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_1.weight', 'lang_encoder.old_decoder_blocks.20.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.20.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_2.weight', 'lang_encoder.old_decoder_blocks.20.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.20.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_1.weight', 'lang_encoder.old_decoder_blocks.21.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.21.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_2.weight', 'lang_encoder.old_decoder_blocks.21.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.21.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_1.weight', 'lang_encoder.old_decoder_blocks.22.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.22.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_2.weight', 'lang_encoder.old_decoder_blocks.22.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.22.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_1.weight', 'lang_encoder.old_decoder_blocks.23.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.23.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_2.weight', 'lang_encoder.old_decoder_blocks.23.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.23.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_1.weight', 'lang_encoder.old_decoder_blocks.24.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.24.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_2.weight', 'lang_encoder.old_decoder_blocks.24.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.24.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_1.weight', 'lang_encoder.old_decoder_blocks.25.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.25.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_2.weight', 'lang_encoder.old_decoder_blocks.25.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.25.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_1.weight', 'lang_encoder.old_decoder_blocks.26.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.26.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_2.weight', 'lang_encoder.old_decoder_blocks.26.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.26.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_1.weight', 'lang_encoder.old_decoder_blocks.27.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.27.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_2.weight', 'lang_encoder.old_decoder_blocks.27.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.27.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_1.weight', 'lang_encoder.old_decoder_blocks.28.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.28.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_2.weight', 'lang_encoder.old_decoder_blocks.28.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.28.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_1.weight', 'lang_encoder.old_decoder_blocks.29.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.29.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_2.weight', 'lang_encoder.old_decoder_blocks.29.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.29.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_1.weight', 'lang_encoder.old_decoder_blocks.30.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.30.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_2.weight', 'lang_encoder.old_decoder_blocks.30.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.30.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_1.weight', 'lang_encoder.old_decoder_blocks.31.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.31.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_2.weight', 'lang_encoder.old_decoder_blocks.31.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.31.ffn.down_proj.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.27.attn_gate', 'lang_encoder.gated_cross_attn_layers.27.ff_gate', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.27.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.27.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.31.attn_gate', 'lang_encoder.gated_cross_attn_layers.31.ff_gate', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.31.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.31.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "    cross_attn_every_n_layers=4\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "model.to(device, torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-64c68aa07984d39f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "vali_dataset = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation')\n",
    "data = vali_dataset.filter(lambda x: x['q_a_pairs'] is not None and len(x['q_a_pairs']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "WARNING:datasets.builder:Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed-graph-only-qa-c5897d2f1995d1be/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "context_data = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation[100:]')\n",
    "first_100 = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation[:100]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file', 'id', 'caption', 'conversations', 'first_mention', 'response', 'title', 'abstract', 'q_a_pairs'],\n",
       "    num_rows: 2902\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "def get_input_example_for_contextual_lerning(context_data, num_examples):\n",
    "    # Pick num_examples random examples after 100\n",
    "    #example_index = random.randint(0, len(context_data), num_examples)\n",
    "    example_indexes = random.sample(range(len(context_data)), num_examples)\n",
    "    questions = []\n",
    "    answers = []\n",
    "    img_paths = []\n",
    "    image_root_folder = '/home/ubuntu/imgs/train/'\n",
    "    for example_idx in example_indexes:\n",
    "        example = context_data[example_idx]\n",
    "        question = example['q_a_pairs'][0][0]\n",
    "        answer = example['q_a_pairs'][0][1]\n",
    "        img_path = image_root_folder + example['image_file']\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        img_paths.append(img_path)\n",
    "    return questions, answers, img_paths\n",
    "\n",
    "\n",
    "def get_input(example):\n",
    "    question = example['q_a_pairs'][0][0]\n",
    "    image_root_folder = '/home/ubuntu/imgs/train/'\n",
    "    image_filepath = example['image_file']\n",
    "    return question, image_root_folder + image_filepath\n",
    "\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "\n",
    "def generate_text(example, num_examples):\n",
    "    \"\"\"\n",
    "    Step 0: pick num_examples random examples\n",
    "    \"\"\n",
    "    Step 1: Load images\n",
    "    \"\"\"\n",
    "    questions, answers, img_paths = get_input_example_for_contextual_lerning(data, num_examples)\n",
    "    demo_examples = [f\"question: {q} answer: {a}\" for q, a in zip(questions, answers)]\n",
    "    demo_images = [Image.open(img_path) for img_path in img_paths]\n",
    "    # Step 1: Load query image\n",
    "    question, img_path = get_input(example)\n",
    "    query_image = Image.open(img_path)\n",
    "    # query = json.dumps({\"question:\": question, \"answer:\": ''})\n",
    "    query = f\"question: {question} answer: \"\n",
    "    \"\"\"\n",
    "    Step 2: Preprocess images\n",
    "    Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    "    batch_size x num_media x num_frames x channels x height x width. \n",
    "    In this case batch_size = num_examples + 1, num_media = 1, num_frames = 1,\n",
    "    channels = 3, height = 224, width = 224.\n",
    "    \"\"\"\n",
    "    if num_examples > 0:\n",
    "        vision_x = [image_processor(img).unsqueeze(0) for img in demo_images]\n",
    "        vision_x.append(image_processor(query_image).unsqueeze(0))\n",
    "        vision_x = torch.cat(vision_x, dim=0)\n",
    "        vision_x = vision_x.unsqueeze(1).unsqueeze(0).to(device, torch.float16)\n",
    "    else:\n",
    "        vision_x = image_processor(query_image).unsqueeze(0)\n",
    "        vision_x = vision_x.unsqueeze(1).unsqueeze(0).to(device, torch.float16)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step 3: Preprocess question\n",
    "    Details: In the text we expect an <image> special token to indicate where an image is.\n",
    "    We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    "    portion associated with an image.\n",
    "    \"\"\"\n",
    "\n",
    "    if num_examples == 0:\n",
    "        lang_x = tokenizer(\n",
    "            [f\"<image>{query}\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    else:\n",
    "        lang_x = tokenizer(\n",
    "            [f\"<image>{'<|endofchunk|>'.join(demo_examples)}<|endofchunk|><image>{query}\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"].to(device),\n",
    "        attention_mask=lang_x[\"attention_mask\"].to(device),\n",
    "        max_new_tokens=100,\n",
    "        num_beams=1,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(generated_text[0])\n",
    "    print(\"Generated text: \", output)\n",
    "    return output\n",
    "\n",
    "# generate_text(first_100[3], 10)\n",
    "\n",
    "responses = []\n",
    "with torch.no_grad() and open(\"open_flaming_6shot\", \"w\") as f:\n",
    "    for i in tqdm(range(len(first_100))):\n",
    "        responses.append(generate_text(first_100[i], 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_model_6_shot = [item.rsplit('answer:', 1)[-1] for item in responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' （1）The first graph shows the error in the interpolant as a function of the regularization strength for a fixed number of pages. （2）The second graph shows the error in the interpolant as a function of the number of pages for a fixed regularization strength.<|endofchunk|>',\n",
       " ' The graph shows the probability of error for the first-order and second-order EA decoders, as a function of the number of measurements m. The measurements are damped by i.i.d',\n",
       " ' （1）The CRR of the camera is higher when the camera is located closer to the transmitter. （2）The CRR of the camera is lower when the camera is located farther from the transmitter. （3）The CRR of the camera is higher when the transmitter is located closer to the receiver. （4）The CRR of the camera is lower when the transmitter is located farther from the receiver.<|endofchunk|>',\n",
       " ' \\tThe findings in Figure 8 suggest that the performance of facial landmark detection algorithms can be improved by using a combination of different techniques. For example, the use of a CNN to extract features from the input image can help to improve the accuracy of the landmark detection algorithm.<|endofchunk|>',\n",
       " ' \\xa0The graph in Figure 7 shows the performance of the three measures for detecting direct causal effects. The results show that PTERV is more effective than PSTE and PTE in detecting weak couplings. This is likely due to the fact that PTERV uses a thresholding function to prune the weights, which can lead to a slight decrease in accuracy. However, the testing accuracy of the model trained with PTERV is higher than that of the model trained with PSTE and PTE,',\n",
       " ' \\xa0The CRB line in the graph is used to show the Cramér-Rao bound (CRB) for the parameter estimation of a Gaussian process. The CRB is a lower bound on the variance of the parameter estimation. The CRB line in the graph shows that the variance of the parameter estimation is bounded by the CRB line.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the deterministic solvers, namely, color refinement and choco, have quadratic runtime on random regular graphs. This is because these graphs have n leaves immediately attached to the root, and are asymmetric. Traces, on the other hand, has a special strategy called the trace invariant, which enables it to abort computation for most of the leaves very early, resulting in quite modest quadratic runtime. In particular, it is still able to outperform dejavu on the isomorphic instances of',\n",
       " ' \\xa0The graph shows that the neural network converges to a stable state after a few epochs. This is because the neural network is able to learn the underlying structure of the data and find a good solution.<|endofchunk|>',\n",
       " ' 𝜆1 is an important hyperparameter that can have a significant impact on the performance of the model. By tuning 𝜆1 in fine-grained, we can achieve a balance between the two tasks of recommendation and SSL, resulting in a model that performs well on both tasks.<|endofchunk|>',\n",
       " ' ith the group size increases, the percentage of groups for which a timetable was found decreases. This is because the group size is a measure of the number of agents in a group, and the larger the group size, the more agents there are in a group. This means that the larger the group size, the more difficult it is to find a timetable for the group. question: What does the graph show about the relationship between the number of agents and the percentage of groups for which a tim',\n",
       " ' ǫ−k/k− is the most efficient method for Chiens model, whereas it is unstable for LaunderSharma and JonesLaunder. The \"segregated\" form ǫ−k/k− (ed = 0) is least efficient, whereas a blend of both forms seems to be optimal as a default option for all models.<|endofchunk|>',\n",
       " ' （1）The proposed synthesis method is scalable with respect to the number of configurations. （2）The proposed synthesis method is scalable with respect to the number of variables. （3）The proposed synthesis method is scalable with respect to the number of constraints.<|endofchunk|>',\n",
       " ' 图表显示了在不同的网络环境下，在不同的时间点，用户的QoE的变化情况。这表明，在不同的网络环境下，用户的QoE的变化情况是不同的。这表明，在不同的网络环境下，用户的QoE',\n",
       " ' 图中的图表是用来比较不同的信息类型在不同的信息量下的效果。这些信息类型包括用户的基本信息、用户的行为信息、用户的关系信息和用户的内容信息。图表中的数据来自于用户的基本信息、用户的行为信息、用户的',\n",
       " ' \\xa0The graph shows that the regression quality of the probe nets is affected by the number of training samples. The probe net with more training samples has better regression quality.<|endofchunk|>',\n",
       " ' 𝑃𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘𝑖𝑘',\n",
       " ' ue the curves in Figure 8 to show the time evolution of the temperature and relative humidity for the wall 1 in the linear case. The solid line represents the solution obtained using the Euler implicit method, the dashed line represents the solution obtained using the Dufort-Frankel method, and the dotted line represents the reference solution.<|endofchunk|>',\n",
       " ' 𝑅(𝑡) is the probability that the average energy consumption exceeds a certain threshold α. 𝑅(𝑡) is the probability that the average energy consumption exceeds a certain threshold α. The difference between the two graphs is that the first graph shows the outage probability of RZF and ZF with imperfect CSI, while the second graph shows the outage probability of RZF and ZF with perfect CSI.<|endofchunk|>',\n",
       " ' 图1 shows the performance of the proposed method for a single plant, while 图2 shows the performance of the proposed method for a set of plants.<|endofchunk|>',\n",
       " ' ith the distribution of data in Figures 4 and 5, the data is divided into two groups: the first group is the data used to train the k-NN classifier and the second group is the data used to test the k-NN classifier.<|endofchunk|>',\n",
       " ' 𝑓𝝑(𝒙) is the transition probability in the recall process as a function of what. The recall process is the process of selecting a subset of items from a set of items. The recall process is a process of selecting a subset of items from a set of items. The recall process is a process of selecting a subset of items from a set of items. The recall process is a process of selecting a subset of items from a set of items.',\n",
       " ' \\xa0The main takeaway from this graph is that the proposed algorithm is able to achieve a good trade-off between the tracking MSE and the FC.<|endofchunk|>',\n",
       " ' \\xa0The main takeaway from this graph is that the WUPS score is higher for the architectures that use the CA rules that are more likely to generate oscillatory macroexecutions. This is because the CA rules that are more likely to generate oscillatory macroexecutions are more likely to generate answers that are more precise.<|endofchunk|>',\n",
       " ' （1）The supervised cost function is convex and the unsupervised cost function is non-convex. （2）The supervised cost function has a unique global minimum at the ground truth solution. （3）The unsupervised cost function has multiple local minima. （4）The supervised cost function has a lower value than the unsupervised cost function at the ground truth solution. （5）The unsupervised cost function has a lower value than the supervised cost function at the ground truth solution',\n",
       " ' \\xa0The graph shows that the proposed global LSTM with GP local model outperforms the other two models in terms of the average accuracy. This is evident from the fact that the proposed model has the highest average accuracy for all three datasets.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the performance of stochastic algorithms with various minibatch sizes is similar. This is because the stochastic algorithms are able to achieve a similar level of performance with a small minibatch size.<|endofchunk|>',\n",
       " ' \\xa0The graph in Figure 13.2 shows the number of instances solved by each solver in a certain wall clock time. The results show that modprep can significantly improve the performance of all four solvers, especially on unsatisfiable instances. This is because modprep can prune the search space by identifying and removing instances that are unlikely to be satisfiable. This allows the solvers to focus on solving only the most promising instances, which leads to a significant improvement in their performance. question: What is',\n",
       " ' \\xa0The graph shows the distribution of the number of words in the PlosOne database. The graph shows that the distribution is approximately exponential, with a mean of around 10,000 words. This is to be expected, as the number of words in a document is proportional to the number of words in the document.<|endofchunk|>',\n",
       " ' \\xa0The purpose of the graph is to show the performance of the proposed EM-Watson method for clustering data from the Watson distribution. The graph shows the mean reciprocal rank of the correct relationship as a function of the number of structured examples. The results demonstrate that the proposed EM-Watson method achieves better performance than the modified K-means method with ML estimator.<|endofchunk|>',\n",
       " ' \\xa0The graph shows the reconstruction error of the different methods for sampling the first time step in Fig. 9 (right). Since the original dataset already contains well distributed samples, as a result of the SPH simulation, the Poisson disk sampling can correctly sample the dataset even for large sample counts. Still, the void-and-cluster techniques consistently lead to the lowest error. The decreased error of stratified kd-tree sampling is likely due to the fact that it is able to better capture the local structure',\n",
       " ' The graph in Figure 7 shows the average decoding delay of different policies versus the number of active users. The two policies considered are the one to reduce',\n",
       " ' \\xa0The main idea of the graph is to compare the accuracy of the parameter estimates obtained by the simple binning approach for two different datasets. The first dataset consists of two-state HMMs with three possible emissions, while the second dataset consists of two-state HMMs with six possible emissions. The graph shows that the simple binning approach performs better for the two-state HMMs with six possible emissions than for the two-state HMMs with three possible emissions. This is',\n",
       " ' （1）The first graph shows the output of the neural network when the input is a single image. （2）The second graph shows the output of the neural network when the input is a 4-D light field.<|endofchunk|>',\n",
       " ' （1）The graph shows that the recognition accuracy decreases as the noise level increases. This is because as the noise level increases, the features become more noisy and less discriminative, making it more difficult for the classifier to correctly classify the frames. （2）The graph shows that the recognition accuracy increases as the number of features increases. This is because as the number of features increases, the classifier has more discriminative features to use for classification.<|endofchunk|>',\n",
       " ' \\xa0The sampling in the graph is to show the performance of the model on the test set. The model is trained on the training set and tested on the test set. The test set is used to evaluate the performance of the model.<|endofchunk|>',\n",
       " ' ρ is the ratio of the number of correct predictions to the total number of predictions. The proposed algorithm has the highest value of ρ, which means that it has the highest accuracy. The proposed algorithm also has the highest value of ρ, which means that it has the highest accuracy. The proposed algorithm has the highest value of ρ, which means that it has the highest accuracy. The proposed algorithm has the highest value of ρ, which means that it has the highest accuracy. The proposed',\n",
       " ' \\xa0The graph shows the average accuracy of the proposed transfer learning method on the target domain data. The accuracy is measured in terms of the mean square error (MSE) of the predictions made by the proposed transfer learning method. The MSE is a measure of the accuracy of the predictions, and a lower MSE value indicates that the predictions are more accurate. The graph shows that the proposed transfer learning method can significantly improve the accuracy of predictions on the target domain data.<|endofchunk|>',\n",
       " ' ue2 is the time it takes for the user to send a message to the server. The time it takes for the server to send a message back to the user is called ue1. The time it takes for the user to send a message to the server and receive a message back from the server is called ue3. The time it takes for the user to send a message to the server and receive a message back from the server is called ue4. The time it takes for',\n",
       " ' \\xa0The learning sample is a set of vertices that are used to learn the predictive distribution of the model. The learning sample is chosen by the decision-maker, which is a neural network that is used to make predictions about the data. The decision-maker takes as input the predictive distribution of the model, which is shown in the red shaded area. The decision-maker then outputs the upper quantile of the data distribution that minimizes the risk. question: What is the purpose of the graph? answer',\n",
       " ' \\xa0The graph shows that the model performance is not significantly affected by the number of GNN hops. This is because the GNN hops are used to propagate the information from the input to the output of the model, and the information is not used to propagate the information from the output to the input of the model.<|endofchunk|>',\n",
       " ' The graph shows the performance of',\n",
       " ' 𝑛 is the number of clients, and 𝑛𝑛 is the number of clients in the system. The deterministic equivalents are the number of clients in the system that are equivalent to the number of clients in the system. For example, if 𝑛𝑛 = 𝑛, then all clients are equivalent to each other.<|endofchunk|>',\n",
       " ' \\xa0The graph in Figure 4 shows the error norm of the approximation by truncated DMD as a function of the number of rows sampled. The error norm is computed by taking the Frobenius norm of the difference between the true and approximated eigenvalues. The graph shows that the error norm decreases as the number of rows sampled increases.<|endofchunk|>',\n",
       " ' ith the number of constrained dimensions k, the proposed NOMA with blanket scheme achieves the highest average max-min value, followed by the proposed NOMA with single selection scheme. The conventional NOMA scheme achieves a lower average max-min value, while the JT-NOMA scheme achieves the lowest average max-min value.<|endofchunk|>',\n",
       " ' 𝑃 is the noise level, and 𝑃 is the noise level. The upper and lower bounds are derived from the bounds on the spectral gap of the SDR (see (5.2) for the spectral gap). The upper bound is derived from the spectral gap of the SDR, while the lower bound is derived from the spectral gap of the SDR. The upper and lower bounds are tight when 𝑃 is small.<|endofchunk|>',\n",
       " ' 𝑓(𝑡) is the complexity function of the trapezoidal word w = aaababa. The complexity function is a measure of the number of states that a word can be in. The complexity function of a word is the number of states that the word can be in. The complexity function of a word is the number of states that the word can be in. The complexity function of a word is the number of states that the word can be in. The complexity',\n",
       " ' 【The graph shows that the proposed fifth-degree SIF (SI 5) with third-degree SIF (SIF3), third-degree CKF (CKF3), fifth-degree CKF (CKF5), and fifth-degree QSIF (QSIF5) for q = 2 and q = 4. What can be inferred from the graph?】 The graph shows that the proposed fifth-degree SIF (SI 5) with third-degree SIF',\n",
       " ' 𝐴𝐵𝐶𝐷𝐸𝐹𝐺𝐻𝐼𝐽𝐾𝐿𝐼𝐽𝐾𝐿𝐼𝐽𝐾𝐿𝐼𝐽𝐾𝐿𝐼',\n",
       " ' \\xa0The graph shows the estimated export scores for the three methods. The estimated export scores are initially very small, but then increase as λ is increased. This is because the ranking lasso penalizes the coefficients of the export scores, so as λ is increased, the coefficients are penalized more heavily and the estimates of the export scores decrease.<|endofchunk|>',\n",
       " ' ue_state_symbol_plot.png shows the state-symbol plot for the UE. The state-symbol plot is used to show the state transition of the UE. The state-symbol plot is a graphical representation of the state transition of the UE. The state-symbol plot is a graphical representation of the state transition of the UE. The state-symbol plot is a graphical representation of the state transition of the UE. The state-symbol plot is a graphical representation of the state transition of',\n",
       " ' \\xa0The main message of the graph is that the S3TA-16-30 model is more effective at defending against adversarial attacks than DENOISE. This is because the S3TA-16-30 model has a lower attack success rate, about 25% lower than DENOISE while nominal accuracy is similar. This suggests that the S3TA-16-30 model is more effective at defending against adversarial attacks than DENOISE.<|endofchunk|>',\n",
       " ' 图中显示了在不同的时间步长上的收敛性。 The graph shows the convergence of the proposed algorithm for different time steps.<|endofchunk|>',\n",
       " ' 图表显示了在不同的精度下，模型在训练过程中的训练速度。 图表显示了在不同的精度下，模型在训练过程中的训练速度。 图表显示了在不同的精度下，模型在训练过程中的训练',\n",
       " ' \\xa0The graph shows that the viscous damping increases as the resolution increases. This is because the higher the resolution, the more accurate the velocity field is.<|endofchunk|>',\n",
       " ' \\xa0The graph shows the evolution of the total free energy and the history of the time step size in the three dimensional test case. The total free energy decreases monotonically as the solution evolves to the steady state, and the time step size is successfully adjusted from ∆tmin to ∆tmax by the NKS solver.<|endofchunk|>',\n",
       " ' \\xa0The red, blue, and green lines in the graph represent the average task completion time, average effort exerted, and average phone orientation change, respectively, for each of the three groups of users. The red line represents the group of users with no experience, the blue line represents the group of users with 1-2 years of experience, and the green line represents the group of users with 3-4 years of experience. The red, blue, and green lines are all increasing with experience, which',\n",
       " ' \\xa0The main goal of the graph is to show the relationship between the number of iterations and the number of data points seen by the model.<|endofchunk|>',\n",
       " ' \\xa0The two graphs in Figure 11 show the total energy requirements of the household in terms of its present value of total consumption (i.e., with respect to period 1) and its present value of total energy requirements (i.e., with respect to period 2). The total energy requirements of the household in terms of its present value of total consumption (i.e., with respect to period 1) is given by the black line in the graph. This line represents the budget constraint of the household',\n",
       " \" \\xa0The graph shows the performance of the different recommendation approaches in terms of the nDCG and UC metrics. The recommendation quality can be improved if the users' social data is provided in addition to the marketplace data. This is because the recommenders based on the users' interactions perform best (CCFs and CFin) but are also significantly depended on the number of social profiles in the dataset.<|endofchunk|>\",\n",
       " ' \\xa0The graph shows that the PDP estimates are highly variable, which can lead to erroneous conclusions about the true effect of a feature. This is because the PDP estimates are based on a single sample, which is subject to sampling error. The graph also shows that the PDP estimates are highly variable, which can lead to erroneous conclusions about the true effect of a feature. This is because the PDP estimates are based on a single sample, which is subject to sampling error. The graph also shows',\n",
       " ' xthe two main axes of the graph are the noise level and the number of iterations.<|endofchunk|>',\n",
       " ' The purpose of the graph is to compare the trajectories created by the validated model and the game theoretical modeling approach for sample encounter number 5. The validated model is based on',\n",
       " ' The graph is used to illustrate the performance of the proposed method in terms of the accuracy and stability of the learned causal structure. The proposed method is able to recover',\n",
       " ' The graph shows the throughput of the proposed algorithm as a function of time. The throughput is measured in bits/second. The graph shows that the throughput of the proposed algorithm is relatively stable over time.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the average travel time for the mesoscopic model is significantly higher than the average travel time for the microscopic model. This is because the mesoscopic model is more complex and requires more computational resources to run.<|endofchunk|>',\n",
       " ' \\xa0The purpose of the graph is to compare the performance of different methods for the recovery of missing entries in a matrix. The graph shows that the proposed OL-KFMC method outperforms other methods significantly in terms of recovery error. This is evident from the fact that the OL-KFMC curve is consistently below the curves of other methods for all values of missing rate. This suggests that OL-KFMC is more effective at recovering missing entries in a matrix than other methods.<|endofchunk|>',\n",
       " ' （1）The graph in Figure 9(a) shows the average trajectory returns of the system with oracle reward function and oracle user policy when the λ value is small. （2）The graph in Figure 9(b) shows the average trajectory returns of the system with oracle reward function and oracle user policy when the λ value is large.<|endofchunk|>',\n",
       " ' 𝑓𝑎𝑙𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟',\n",
       " ' ith vehicle dynamics in the graph shows the vehicle dynamics of the fifth vehicle in the simulation. The vehicle dynamics is the vehicle’s acceleration and velocity. The vehicle dynamics is calculated by the vehicle dynamics model. The vehicle dynamics model is a model that describes the vehicle’s acceleration and velocity. The vehicle dynamics model is used to calculate the vehicle dynamics. The vehicle dynamics model is used to calculate the vehicle dynamics. The vehicle dynamics model is used to calculate the vehicle dynamics. The vehicle dynamics model is used',\n",
       " ' \\xa0The 53.05 dBm value in the graph is the average power of the signal received by the detector. The higher the average power, the more likely the detector is to detect the signal.<|endofchunk|>',\n",
       " ' \\tThe main takeaways from the graph are that the time series x(n) passing the ETS is transformed into a new time series y(n), and that the new time series y(n) is a low-pass filtered version of the original time series x(n).<|endofchunk|>',\n",
       " ' \\xa0The main goal of the experiment is to show that the proposed method can be used to detect memory exhaustion attacks. The x-axis shows the time in hours, and the y-axis shows the amount of memory exhausted in KIB. The different lines on the graph represent different values of τ, which is the time interval in which the adversary chooses to launch his attacks.<|endofchunk|>',\n",
       " ' \\tThe key takeaways from the graph are as follows:\\n\\n* The AI increases as the grid is refined, and the TTS also increases as the grid is refined. This is because the more refined the grid, the more computational work is required to solve the problem. * The AI is higher for the finer grids, but the TTS is higher for the coarser grids. This is because the finer grids require more computational work to solve the problem, but the coarser grids have a',\n",
       " ' The graph shows the evolution of the state of the system over time. The state of the system is represented by the blue line, which shows the',\n",
       " ' \\xa0The graph shows the relationship between the number of iterations and the number of samples. It can be seen that the number of iterations increases as the number of samples increases. This is because the number of iterations is proportional to the number of samples, and as the number of samples increases, the number of iterations also increases.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the performance of the three learning algorithms is similar. This is because the three algorithms are all based on the same learning rule.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the posterior distribution of model predictions is well-centered around the maximum likelihood point. This suggests that the RF is able to accurately estimate the model parameters.<|endofchunk|>',\n",
       " ' \\tThe graph shows the relationship between the normalized squared error and the rank of the matrix. The results show that the normalized squared error increases as the rank increases. This is because the rank of the matrix is a measure of the amount of information that the matrix contains. The more information that the matrix contains, the more difficult it is to recover the matrix from its singular value decomposition.<|endofchunk|>',\n",
       " ' \\xa0The graph shows the mean cumulative reward of the master and base algorithms over 100 runs. The master algorithm is the algorithm that is trained on the master dataset, while the base algorithm is the algorithm that is trained on the base dataset. The master algorithm is able to achieve a higher mean cumulative reward than the base algorithm. This suggests that the master algorithm is able to learn a better policy than the base algorithm.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the agent is able to learn to play the CoinRun game with a high degree of accuracy. This is evident from the fact that the agent is able to achieve a high score on the test set, and that it is able to learn from a small amount of training data.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the life time of pages in WM increases as the number of pages in WM increases. This is because the probability of a page being evicted from WM decreases as the number of pages in WM increases.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the fraction of attacker-free (entry, exit) pairs decreases as the size of the guard set is reduced. This is because the attacker is able to choose a smaller set of trajectories to attack, which reduces the number of trajectories that are not attacked.<|endofchunk|>',\n",
       " ' The graph shows the performance of the algorithm as a function of the number of iterations. The algorithm is initialized with a random policy',\n",
       " ' The graph shows the relationship between the number of corrective demonstrations and the number of deficient demonstrations. The graph shows that the number of corrective demonstrations is higher than the number of deficient demonstrations.',\n",
       " ' \\xa0The graph shows the performance of the ANN model in the Euro Stoxx 50 dataset. The graph shows that the ANN model performs relatively poorly in the Euro Stoxx 50 dataset, with a high prevalence of red points. This is consistent with the results in Table 3 and Figure 7, which show that the ANN model has a lower accuracy and a higher mean absolute error than the linear regression model.<|endofchunk|>',\n",
       " ' \\xa0The graph represents the performance of the proposed method in terms of the number of iterations required to converge to a solution. The proposed method converges to a solution in a significantly smaller number of iterations than the other methods.<|endofchunk|>',\n",
       " ' \\xa0The main goal of the graph is to show the performance of the proposed method for the prediction of the daily maximum temperature. The graph shows that the proposed method outperforms the baseline method in terms of the RMSE and MAE.<|endofchunk|>',\n",
       " ' \\xa0The different colors in the graph represent the different algorithms. The blue line represents the algorithm with the best performance, the red line represents the algorithm with the second best performance, and the green line represents the algorithm with the third best performance.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the enhanced OFDM-SNM scheme outperforms the original OFDM-SNM scheme in terms of the average throughput.<|endofchunk|>',\n",
       " ' 𝑅𝑖𝑛𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟',\n",
       " ' \\xa0The graph shows that FedNAG converges faster than other benchmark algorithms. This is because FedNAG uses a novel gradient estimation technique that is able to estimate the gradient of the loss function with a high degree of accuracy. This allows FedNAG to achieve a high convergence rate.<|endofchunk|>',\n",
       " ' \\xa0The graph shows that the proposed method is able to achieve a higher accuracy than the baseline method. This is likely due to the fact that the proposed method uses a more advanced model for the prediction task, which allows it to better capture the underlying dynamics of the data.<|endofchunk|>',\n",
       " ' \\xa0The graph shows the performance of the different methods as a function of the number of training samples. The performance is measured in terms of the mean squared error (MSE) of the forecasting error. The x-axis shows the number of training samples, and the y-axis shows the MSE. The methods are ordered by the number of parameters they use.<|endofchunk|>',\n",
       " ' The main takeaway from this graph is that the angle of the double pendulum is always greater than the angle of the pendulum. This is because the double pendulum has two DOFs, while the pendulum has',\n",
       " ' \\xa0The graph shows that the trends T8 to T10 are all decreasing. This suggests that the model is getting worse over time',\n",
       " ' ϕ-function is a function that is used to define the dynamics of a system. In this case, the ϕ-function is used to define the dynamics of a harmonic oscillator. The graph shows the effect of randomly sampling points from input video frames on object segmentation IoU of BNN-Identity on DAVIS dataset. The results show that the object segmentation IoU decreases as the number of randomly sampled points increases. This is because the randomly sampled points are not necessarily located on the object, and',\n",
       " ' 𝑓𝑎𝑙𝑙𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒𝑟𝑒',\n",
       " ' \\xa0The buffer size and the future window are used to determine the number of samples that are used to compute the average value of the state. The average value of the state is then used to compute the state estimate. The buffer size and the future window are used to determine the number of samples that are used to compute the average value of the state. The average value of the state is then used to compute the state estimate. The buffer size and the future window are used to determine the number of samples',\n",
       " ' \\xa0The graph shows that the runtime of streaming algorithms is significantly lower than the runtime of Sieve. This is because the streaming algorithms are able to process the data in a streaming fashion, while Sieve requires the entire dataset to be loaded into memory before it can be processed.<|endofchunk|>',\n",
       " ' ρ = 2 is the best choice for the delay mitigation parameter ρ, as it yields the best performance in terms of the average delay. This is important because it means that the proposed delay mitigation strategy is effective in reducing the average delay.<|endofchunk|>']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_model_6_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "8.5\n",
      "0\n",
      "6.5\n",
      "0.0\n",
      "7.5\n",
      "0\n",
      "0.0\n",
      "0\n",
      "0\n",
      "3.0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "8.5\n",
      "3.0\n",
      "5.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "6.5\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "5.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "system_message = \"\"\"\n",
    "You are a helpful and precise assistant for checking the quality of the answer.\n",
    "You are given the graph's caption, the context of the graph, the abstract, tthe title\n",
    "\n",
    "And then you are given the question, the reference answer, and the answer generated by the model. Please\n",
    "think about how helpful the model answer is to the user and rate the model answer on a scale of 0 to 10, \n",
    "where 0 is not helpful at all and 10 is very helpful. Just return the floating number between 0 and 10.\n",
    "\"\"\"\n",
    "\n",
    "def construct_input_string(first_100, index):\n",
    "    content = dict()\n",
    "    cur_example = first_100[index]\n",
    "    content['title'] = cur_example['title']\n",
    "    content['abstract'] = cur_example['abstract']\n",
    "    content['caption'] = cur_example['caption']\n",
    "    content['Question to the model'] = cur_example['q_a_pairs'][0][0]\n",
    "    content['reference_answer'] = cur_example['q_a_pairs'][0][1]\n",
    "    content['Candidate model answer'] = responses_model_6_shot[index]\n",
    "    return json.dumps(content)\n",
    "\n",
    "\n",
    "def get_openai_response(content_string):\n",
    "    openai_response = openai.ChatCompletion.create(\n",
    "                    model='gpt-4',\n",
    "                    messages=[{\n",
    "                        'role': 'system',\n",
    "                        'content': system_message\n",
    "                    }, {\n",
    "                        'role': 'user',\n",
    "                        'content': content_string\n",
    "                    }],\n",
    "                    temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n",
    "                    max_tokens=500,\n",
    "                )['choices'][0]['message']['content']\n",
    "    return openai_response\n",
    "\n",
    "openai_responses = []\n",
    "for i in range(len(data)):\n",
    "    content_string = construct_input_string(first_100, i)\n",
    "    openai_response = get_openai_response(content_string)\n",
    "    print(openai_response)\n",
    "    openai_responses.append(openai_response)\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "openai_responses_float = [float(str) for str in openai_responses]\n",
    "rated_data = data.add_column(\"openflamingo_answer_6_shot\", responses_model_6_shot)\n",
    "rated_data = rated_data.add_column(\"openai_rating\", openai_responses_float)\n",
    "\n",
    "output_file_path = \"openfliamgo_answer_and_openai_rating.jsonl\"\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for example in rated_data:\n",
    "        json_str = json.dumps(example)\n",
    "        f.write(json_str + '\\n')\n",
    "\n",
    "import numpy as np\n",
    "mean, std = np.mean(openai_responses_float), np.std(openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_responses_float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean, std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(openai_responses_float), np\u001b[39m.\u001b[39mstd(openai_responses_float)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(mean, std)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai_responses_float' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean, std = np.mean(openai_responses_float), np.std(openai_responses_float)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean, std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(openai_responses_float), np\u001b[39m.\u001b[39mstd(openai_responses_float)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "mean, st\n",
    "d = np.mean(openai_responses_float), np.std(openai_responses_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_100), len(response_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
