{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9b8be1-481b-41d5-95eb-990dd51e6ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "train = load_from_disk('/home/ubuntu/LLaVA-Graph/with_abstract_graph_derived_question_first_twenty_percent_train')\n",
    "validation_one_percent = load_from_disk('/home/ubuntu/LLaVA-Graph/with_abstract_graph_derived_question_first_one_percent_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc20540b-1630-4bee-a44a-af1beb66d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.push_to_hub('alexshengzhili/SciCapInstructed410K', \n",
    "#                   split='train', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c361867-d9d2-43ce-85f8-eb4579bf50df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is the difference between the two graphs in Figure 10?\\nAnswer: The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.\\n\\nQuestion: What are the key observations that can be made from the graphs?\\nAnswer: The key observations that can be made from the graphs are as follows:\\n\\n* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.\\n* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.\\n* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.\\n\\nQuestion: What are the implications of these observations?\\nAnswer: The first observation suggests that the still-to-video FR systems are able to learn the variations in pose and lighting effects from the synthetic ROIs. The second observation suggests that the SRC classifier is better at learning these variations than the SVM classifier. The third observation suggests that the still-to-video FR system is more robust to pose and lighting variations on the Chokepoint database than on the COX-S2V database.\\n\\nQuestion: What are the limitations of the study?\\nAnswer: One limitation of the study is that it only considers two databases. It would be interesting to see how the still-to-video FR systems perform on other databases with different pose and lighting variations. Another limitation of the study is that it only considers two classifiers. It would be interesting to see how the still-to-video FR systems perform with other classifiers, such as deep learning classifiers.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_one_percent['response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5fefd9d-2bea-4909-962a-32cc380c451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "# def extract_q_a(s):\n",
    "#     pairs = []\n",
    "#     segments = s.split('\\n\\n')\n",
    "\n",
    "#     for i in range(0, len(segments)-1, 2):\n",
    "#         if len(segments[i]) < 1:\n",
    "#             continue\n",
    "#         question = segments[i].replace('Question: ', '').strip()\n",
    "#         answer = segments[i+1].replace('Answer: ', '').strip()\n",
    "#         pairs.append((question, answer))\n",
    "\n",
    "#     return pairs\n",
    "def convert_to_qa_pairs_using_regex(dialog_string):\n",
    "    # Split the dialog string into segments whenever 'Question:' or 'Answer:' appears\n",
    "    segments = re.split('Question: |Answer: ', dialog_string)\n",
    "    \n",
    "    # Remove any empty strings from the list\n",
    "    segments = [segment for segment in segments if (len(segment) > 3 and segment != '\\n')]\n",
    "    \n",
    "    # Group the segments into pairs (question, answer)\n",
    "    qa_pairs = [(segments[i].strip('\\n').strip(), segments[i+1].strip('\\n').strip()) for i in range(0, len(segments)-1, 2)]\n",
    "    valid_pairs = []\n",
    "    for question, answer in qa_pairs:\n",
    "        keywords = ['graph', 'diagram', 'figure', 'chart', 'axis', 'plot', 'table', 'image', 'visual', 'illustrat']\n",
    "        if not any(keyword in question.lower() for keyword in keywords):\n",
    "            continue\n",
    "        valid_pairs.append((question, answer))\n",
    "    return valid_pairs\n",
    "    \n",
    "empty_indexes = []\n",
    "for _ in range(100):\n",
    "    random_intgeter = random.randint(0, 50000)\n",
    "    q_a_pairs = convert_to_qa_pairs_using_regex(train['response'][random_intgeter])\n",
    "    if len(q_a_pairs) < 1:\n",
    "        empty_indexes.append(random_intgeter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f8411b2-ae93-4406-a3e0-b5fa7ecbc4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/LLaVA-Graph/with_abstract_graph_derived_question_first_one_percent_train/cache-a784abaa07f44da5.arrow\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def convert_to_conversations(example, insert_context= False):\n",
    "    q_a_pairs = convert_to_qa_pairs_using_regex(example['response'])\n",
    "    conversations = []\n",
    "    \n",
    "    random_prompt = ''\n",
    "    for i, qa in enumerate(q_a_pairs):\n",
    "        Question, Answer = qa\n",
    "        \n",
    "        # for the first round, use the random prompt\n",
    "        if i == 0:\n",
    "            if insert_context:\n",
    "                Question = example['title'] + ' ' + example['caption'] + ' ' + Question\n",
    "            if random.random() < 0.5:\n",
    "                random_prompt = f\"{Question}\\n<image>\"\n",
    "            else:\n",
    "                random_prompt = f\"<image>\\n{Question}\"\n",
    "            \n",
    "            conversations.append({\"from\": \"human\", \"value\": random_prompt})\n",
    "        else: \n",
    "            # for subsequent rounds, just use the question and answer\n",
    "            conversations.append({\"from\": \"human\", \"value\": Question})\n",
    "        conversations.append({\"from\": \"gpt\", \"value\": Answer})\n",
    "        \n",
    "    return conversations, q_a_pairs\n",
    "\n",
    "def convert_example(example):\n",
    "    conversations, q_a_pairs = convert_to_conversations(example)\n",
    "    example['conversations'] = conversations\n",
    "    example['q_a_pairs'] = q_a_pairs\n",
    "    return example\n",
    "\n",
    "\n",
    "def filter_for_non_helpful_instructions(example):\n",
    "    item = example['response']\n",
    "    if ('Question' not in item or\n",
    "        'Answer' not in item):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "isTrain = False\n",
    "if isTrain: \n",
    "    \n",
    "    # test_train = train[:100]\n",
    "    filtered_train = train.filter(filter_for_non_helpful_instructions)\n",
    "    converted_conversations = filtered_train.map(convert_example)\n",
    "\n",
    "isVal = True\n",
    "if isVal:\n",
    "    filtered_validation = validation_one_percent.filter(filter_for_non_helpful_instructions)\n",
    "    converted_conversations_validation = validation_one_percent.map(convert_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a06c96a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3520"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_conversations_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40a212c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What is the difference between the two graphs in Figure 10?', 'The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.'], ['What are the key observations that can be made from the graphs?', 'The key observations that can be made from the graphs are as follows:\\n\\n* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.\\n* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.\\n* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.']]\n",
      "[['How does the graph illustrate the principle of PPS modulation?', \"The graph illustrates the principle of PPS modulation by showing how the robot's PPS is modulated in response to the different human body parts. In particular, the robot's PPS is attenuated at the hands (i.e. θ = −0.5 in Eq. 1), while it is positively modulated at the head (valence 1.0). This means that the robot is more likely to avoid collisions with the head than with the hands.\"]]\n",
      "[['What are the main takeaways from the graph in Figure 9?', 'The main takeaways from the graph in Figure 9 are as follows:\\n\\n* The runtime of pruned algorithms (IRQ and IRJQ) decreases with an increasing threshold θ.\\n* The recall decreases with the threshold because the larger θ, the fewer trajectories are satisfied.\\n* The accuracy rate still maintains at a relatively high value.\\n\\nThese results verify the efficiency of the pruning strategies used in the paper.']]\n",
      "[['What are the implications of the findings in Figure 8 for the design of facial landmark detection algorithms?', 'The findings in Figure 8 suggest that the standard deviation of Gaussian sampling is an important factor to consider in the design of facial landmark detection algorithms. Appropriate values of the standard deviation can lead to improved accuracy in facial landmark detection.']]\n",
      "[['What does the graph in Figure 7 show?', 'The graph in Figure 7 shows the behavior of variance and cost of the multilevel difference for all parameter combinations. The variance and cost are plotted on the left and right axes, respectively. The x-axis represents the level `, which ranges from 0 to L. The dashed line represents the homogeneous case (a = 0), and the full line represents the heterogeneous case (a 0).'], ['What does the dashed line in Figure 7 represent?', 'The dashed line in Figure 7 represents the homogeneous case, in which all levels have the same variance and cost. This is because in the homogeneous case, the level selection strategy does not affect the variance and cost of the multilevel difference.'], ['What does the full line in Figure 7 represent?', 'The full line in Figure 7 represents the heterogeneous case, in which the levels have different variances and costs. This is because in the heterogeneous case, the level selection strategy can affect the variance and cost of the multilevel difference.'], ['What are the implications of the results in Figure 7?', 'The results in Figure 7 indicate that the level selection strategy can be used to control the variance and cost of the multilevel difference. This can be useful in applications where it is important to minimize the variance or cost of the multilevel difference.']]\n",
      "[['What is the significance of the CRB line in the graph?', 'The CRB line represents the theoretical minimum MSE that can be achieved by any estimator. The fact that the MSEs of the proposed IMLE and IMAPE algorithms are close to the CRB line indicates that they are very accurate estimators.'], ['What are the implications of the results in this graph?', 'The results in this graph show that the proposed IMLE and IMAPE algorithms are very accurate estimators of the parameter θ. This means that they can be used to estimate θ with high precision, even in the presence of noise.']]\n",
      "[['What is the main takeaway from the graph?', 'The main takeaway from the graph is that as the number of objects increases, throughput decreases, and query response time increases for both range queries and kNN queries. This is because the space-filling curves used in Bx-trees can cause \"false hits\", which reduce query utility. On the contrary, TPR*-trees have more complicated update operations which make query more efficient at a sacrifice of throughput. Finally, SP indexes consistently outperform other indexes in all settings.']]\n",
      "[['What does the graph show about the training history of the neural network?', 'The graph shows that the neural network was able to learn the relationship between terrain texture and log(arsenic) in stream sediments. The mean-squared-error (MSE) on the held out test data decreased over time, indicating that the network was becoming more accurate. The best epoch was found to be around 200, after which the MSE began to increase again as the network began to overfit.']]\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for i in range(len(converted_conversations_validation)):\n",
    "    cur_length = len(converted_conversations_validation[i]['q_a_pairs'])\n",
    "    length.append(cur_length)\n",
    "    if i // 8 == 0:\n",
    "        print(converted_conversations_validation[i]['q_a_pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b7a328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 15.84ba/s]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "Downloading metadata: 100%|██████████| 610/610 [00:00<00:00, 6.05MB/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "converted_conversations_validation = converted_conversations_validation.filter(lambda x: len(x['q_a_pairs']) > 0)\n",
    "\n",
    "converted_conversations_validation.push_to_hub('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2128710-aea3-4937-8710-2b1937935edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What is the difference between the two graphs in Figure 10?', 'The two graphs in Figure 10 show the performance of still-to-video FR systems on the Chokepoint and COX-S2V databases, respectively. The x-axis of both graphs shows the number of synthetic ROIs generated under both pose and lighting effects, while the y-axis shows the average AUC and AUPR.'], ['What are the key observations that can be made from the graphs?', 'The key observations that can be made from the graphs are as follows:\\n\\n* The performance of both still-to-video FR systems improves as the number of synthetic ROIs increases.\\n* The performance of the still-to-video FR system with the SRC classifier is better than the performance of the still-to-video FR system with the SVM classifier.\\n* The performance of the still-to-video FR system on the Chokepoint database is better than the performance of the still-to-video FR system on the COX-S2V database.']]\n",
      "[['How does the graph illustrate the principle of PPS modulation?', \"The graph illustrates the principle of PPS modulation by showing how the robot's PPS is modulated in response to the different human body parts. In particular, the robot's PPS is attenuated at the hands (i.e. θ = −0.5 in Eq. 1), while it is positively modulated at the head (valence 1.0). This means that the robot is more likely to avoid collisions with the head than with the hands.\"]]\n",
      "[['What are the main takeaways from the graph in Figure 9?', 'The main takeaways from the graph in Figure 9 are as follows:\\n\\n* The runtime of pruned algorithms (IRQ and IRJQ) decreases with an increasing threshold θ.\\n* The recall decreases with the threshold because the larger θ, the fewer trajectories are satisfied.\\n* The accuracy rate still maintains at a relatively high value.\\n\\nThese results verify the efficiency of the pruning strategies used in the paper.']]\n",
      "[['What are the implications of the findings in Figure 8 for the design of facial landmark detection algorithms?', 'The findings in Figure 8 suggest that the standard deviation of Gaussian sampling is an important factor to consider in the design of facial landmark detection algorithms. Appropriate values of the standard deviation can lead to improved accuracy in facial landmark detection.']]\n",
      "[['What does the graph in Figure 7 show?', 'The graph in Figure 7 shows the behavior of variance and cost of the multilevel difference for all parameter combinations. The variance and cost are plotted on the left and right axes, respectively. The x-axis represents the level `, which ranges from 0 to L. The dashed line represents the homogeneous case (a = 0), and the full line represents the heterogeneous case (a 0).'], ['What does the dashed line in Figure 7 represent?', 'The dashed line in Figure 7 represents the homogeneous case, in which all levels have the same variance and cost. This is because in the homogeneous case, the level selection strategy does not affect the variance and cost of the multilevel difference.'], ['What does the full line in Figure 7 represent?', 'The full line in Figure 7 represents the heterogeneous case, in which the levels have different variances and costs. This is because in the heterogeneous case, the level selection strategy can affect the variance and cost of the multilevel difference.'], ['What are the implications of the results in Figure 7?', 'The results in Figure 7 indicate that the level selection strategy can be used to control the variance and cost of the multilevel difference. This can be useful in applications where it is important to minimize the variance or cost of the multilevel difference.']]\n",
      "[['What is the significance of the CRB line in the graph?', 'The CRB line represents the theoretical minimum MSE that can be achieved by any estimator. The fact that the MSEs of the proposed IMLE and IMAPE algorithms are close to the CRB line indicates that they are very accurate estimators.'], ['What are the implications of the results in this graph?', 'The results in this graph show that the proposed IMLE and IMAPE algorithms are very accurate estimators of the parameter θ. This means that they can be used to estimate θ with high precision, even in the presence of noise.']]\n",
      "[['What is the main takeaway from the graph?', 'The main takeaway from the graph is that as the number of objects increases, throughput decreases, and query response time increases for both range queries and kNN queries. This is because the space-filling curves used in Bx-trees can cause \"false hits\", which reduce query utility. On the contrary, TPR*-trees have more complicated update operations which make query more efficient at a sacrifice of throughput. Finally, SP indexes consistently outperform other indexes in all settings.']]\n",
      "[['What does the graph show about the training history of the neural network?', 'The graph shows that the neural network was able to learn the relationship between terrain texture and log(arsenic) in stream sediments. The mean-squared-error (MSE) on the held out test data decreased over time, indicating that the network was becoming more accurate. The best epoch was found to be around 200, after which the MSE began to increase again as the network began to overfit.']]\n",
      "[['What are the implications of the sharp points in the graph?', 'The sharp points in the graph correspond to the change of achievability scheme for different values of INR. As INR increases, the achievable scheme changes from one that uses only the first two power levels to one that uses all four power levels. This change in achievability scheme results in a sharp increase in the gap between the achievable scheme and the outer-bounds.'], ['What are the key takeaways from the graph?', 'The key takeaways from the graph are that the proposed achievable scheme achieves high sum-rates, with a maximum loss of 4, 5, and 5.5 bits/sec/Hz for SNR = 20dB, 40dB, and 60dB, respectively. The sharp points in the graph correspond to the change of achievability scheme for different values of INR.']]\n",
      "[['What does the graph show about the relationship between group size and the percentage of groups for which a timetable was found?', \"The graph shows that as the group size increases, the percentage of groups for which a timetable was found decreases. This is because as the group size increases, the number of possible combinations of train and coach schedules increases exponentially, making it more difficult to find a timetable that satisfies all of the group's constraints.\"], ['What are some of the factors that contribute to the difficulty of finding a timetable for large groups?', \"There are several factors that contribute to the difficulty of finding a timetable for large groups. First, as the group size increases, the number of possible combinations of train and coach schedules increases exponentially. This makes it more difficult to find a timetable that satisfies all of the group's constraints. Second, large groups often have more complex constraints, such as requiring specific train connections or coaches. This further complicates the task of finding a timetable that meets all of the group's needs.\"], ['What are some of the implications of the findings in this graph?', 'The findings in this graph have several implications. First, they suggest that it is more difficult to find a timetable for large groups than for small groups. This is important for tour operators and travel agents to keep in mind when planning trips for large groups. Second, the findings suggest that large groups often have more complex constraints, which makes it even more difficult to find a timetable that meets all of their needs. This is important for tour operators and travel agents to be aware of when planning trips for large groups.']]\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for i in range(len(converted_conversations_validation)):\n",
    "    cur_length = len(converted_conversations_validation[i]['q_a_pairs'])\n",
    "    length.append(cur_length)\n",
    "    if i // 10 == 0:\n",
    "        print(converted_conversations_validation[i]['q_a_pairs'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96be11bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file', 'id', 'caption', 'conversations', 'first_mention', 'response', 'title', 'abstract', 'q_a_pairs'],\n",
       "    num_rows: 3002\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_conversations_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6fc773c-e5bc-487f-b0a9-2a641a27bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1024.,  847.,    0.,  623.,    0.,  415.,   78.,    0.,   13.,\n",
       "           2.]),\n",
       " array([1. , 1.6, 2.2, 2.8, 3.4, 4. , 4.6, 5.2, 5.8, 6.4, 7. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9bf52a63-4311-486e-a5bd-6eb11974a952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 2.241505662891406)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(length), np.mean(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afc54dcd-c9b8-4525-b329-bda33a328331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_conversations[180]['conversations'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24959b0-0d9a-4a3a-9554-797ce6f4d7c6",
   "metadata": {},
   "source": [
    "# Eval construction only/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "38e58e4e-b7e7-4aed-b717-94a38ca79207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/generative-ai/validation_with_response_fix_onepercenttest/cache-c35e1a157f242711.arrow\n",
      "Loading cached processed dataset at /home/jupyter/generative-ai/validation_with_response_fix_onepercenttest/cache-2bf4b118c5d958a0.arrow\n"
     ]
    }
   ],
   "source": [
    "def convert_to_only_question(example):\n",
    "    Question, _ = extract_q_a(example['response'])\n",
    "    example[\"text\"]= Question\n",
    "    example[\"category\"] = \"conv\"\n",
    "    return example\n",
    "\n",
    "filtered_validation = validation_one_percent.filter(filter_for_non_helpful_instructions)\n",
    "validation_converted = filtered_validation.map(convert_to_only_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e7fdece9-61f3-426b-a92f-ffda9de19917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:00<00:00, 9365.50it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_converted_for_eval = []\n",
    "\n",
    "# Iterate over the dataset\n",
    "for record in tqdm(validation_converted):\n",
    "    # Apply some processing function to your data, if needed\n",
    "    # processed_data = process_numpy(record)\n",
    "\n",
    "    # If no processing is needed, directly append the record\n",
    "    processed_data = dict(image=record['image_file'], text=record['text'], category=record['category'])\n",
    "\n",
    "    # Append the data to the list\n",
    "    validation_converted_for_eval.append(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6d6e9a5e-9d05-4f16-b2c2-df7fd2c914fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also construct the 'detail' questions based on 1% of validation set\n",
    "\n",
    "validation = load_from_disk('validation_with_response_fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cc39ce15-fead-47d0-9147-547ec31945ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_last_one_percent = validation[9100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7139af19-860c-400c-b11b-23f5302e0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_prompt_list = [\n",
    "    \"Describe the following image in detail\",\n",
    "    \"Provide a detailed description of the given image\",\n",
    "    \"Give an elaborate explanation of the image you see\",\n",
    "    \"Share a comprehensive rundown of the presented image\",\n",
    "    \"Offer a thorough analysis of the image\",\n",
    "    \"Explain the various aspects of the image before you\",\n",
    "    \"Clarify the contents of the displayed image with great detail\",\n",
    "    \"Characterize the image using a well-detailed description\",\n",
    "    \"Break down the elements of the image in a detailed manner\",\n",
    "    \"Walk through the important details of the image\",\n",
    "    \"Portray the image with a rich, descriptive narrative\",\n",
    "    \"Narrate the contents of the image with precision\",\n",
    "    \"Analyze the image in a comprehensive and detailed manner\",\n",
    "    \"Illustrate the image through a descriptive explanation\",\n",
    "    \"Examine the image closely and share its details\",\n",
    "    \"Write an exhaustive depiction of the given image\"\n",
    "]\n",
    "\n",
    "import random \n",
    "list_of_deatil_prompt = []\n",
    "for i in range(len(validation_last_one_percent['image_file'])):\n",
    "    image = validation_last_one_percent['image_file'][i]\n",
    "    prompt = random.choice(detailed_prompt_list)\n",
    "    list_of_deatil_prompt.append(dict(image=image, text=prompt, category=\"detail\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f24ca1ce-5eb0-4f32-99b8-ed5447294a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = validation_converted_for_eval + list_of_deatil_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6d564e4-c129-4f30-b7fd-7bd710045d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(combined_list)):\n",
    "    combined_list[i]['question_id'] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03281fac-5953-4c95-9dc5-6ed8421ea8af",
   "metadata": {},
   "source": [
    "# This saves the dataset to proper json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8374ed14-6174-497d-a825-14f613d7a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56520/56520 [00:08<00:00, 6612.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "def convert_dataset_into_json(converted_conversations, json_filename):\n",
    "    # Initialize a list to save processed data\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for record in tqdm(converted_conversations):\n",
    "        # Apply some processing function to your data, if needed\n",
    "        # processed_data = process_numpy(record)\n",
    "\n",
    "        # If no processing is needed, directly append the record\n",
    "        processed_data = record\n",
    "\n",
    "        # Append the data to the list\n",
    "        all_data.append(processed_data)\n",
    "\n",
    "    # Save all the data to a line-separated JSON file\n",
    "    with open(json_filename, \"w\") as output_file:\n",
    "        json.dump(all_data, output_file, indent=4)\n",
    "            \n",
    "# convert_dataset_into_json(combined_list, 'data/eval.json')\n",
    "convert_dataset_into_json(converted_conversations, 'first_twenty_dictionary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "002e4fb5-0501-4de0-ac67-eed7fa6b27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 1374829.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use for train\n",
    "\n",
    "convert_dataset_into_json(combined_list, 'data/eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29f3eb-165f-40de-839f-669b0e90f2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
