{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97109dbc-2d2d-4d8d-9f1c-22c4cfce931e",
   "metadata": {},
   "source": [
    "Count letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba13166-a466-4f89-9d6e-9add5db13586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.6.3 tiktoken-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4563cc-67fc-4728-a49d-5c5221b1ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ff728c-e77b-49fc-ac73-64262c1b010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += len(encoding.encode(str(message)))\n",
    "        num_tokens += tokens_per_message\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368aed21-4f76-4256-904f-e5a04cb855e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "first_twenty = load_from_disk('/home/jupyter/generative-ai/with_abstract_train_first_twenty_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6bbeb6-d98a-4f24-9e22-a30e1403612b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file', 'id', 'caption', 'conversations', 'first_mention', 'response', 'title', 'abstract'],\n",
       "    num_rows: 70404\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_twenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dfcc9d6-978d-4795-a87d-53d4ef2f21d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is the x-axis of the graph?\\n\\nAnswer: The x-axis of the graph represents the number of synthetic ROIs generated with DSFS, 3DMM, and SHBMM according to pose and lighting effects.\\n\\nQuestion: What is the y-axis of the graph?\\n\\nAnswer: The y-axis of the graph represents the average AUC and AUPR obtained by increasing the number of synthetic ROIs used to design SRC and SVM classifiers on the Chokepoint and COX-S2V databases.\\n\\nQuestion: What does the graph show?\\n\\nAnswer: The graph shows that the performance of still-to-video FR systems improves as the number of synthetic ROIs increases. This is because the synthetic ROIs help to capture the variations in pose and lighting that are present in the operational domain.\\n\\nQuestion: What are the key takeaways from the graph?\\n\\nAnswer: The key takeaways from the graph are that:\\n\\n1. The performance of still-to-video FR systems can be improved by augmenting the reference set with synthetic faces.\\n2. The number of synthetic ROIs that need to be generated depends on the amount of variation in pose and lighting that is present in the operational domain.\\n3. The proposed DSFS approach can be used to generate synthetic faces that resemble individuals of interest under the capture conditions relevant to the operational domain.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_twenty['response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b00a53-3d8a-49be-ba8f-6329af11dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = num_tokens_from_messages(first_twenty['response']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb02530-f4b9-4393-8c0b-434787c703f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = num_tokens_from_messages(first_twenty['conversations']) + num_tokens_from_messages(first_twenty['abstract']) + num_tokens_from_messages(first_twenty['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b17bf1a5-faa2-448a-9846-0cadf7fd762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "      You are an AI visual assistant that can analyze a graph in a scientific paper. You are provided with the OCR-extracted text, the caption of the figure, and the first paragraph that mentioned the figure.\n",
    "\n",
    "      Your task is to use information from all these provided sources to create a plausible question about the graph, and then provide a detailed answer.\n",
    "\n",
    "      You should aim to ask complex questions that go beyond a simple description of the graph. The answer to such questions should require understanding the graph data, and then reasoning based on background knowledge or interpretation. You should aim to provide guides and scholarly perspectives based on the graph's data and context.\n",
    "\n",
    "      Avoid directly revealing specific details in the question. Make the question challenging in a way that the user needs to first reason about the graph data and the context they have derived from the accompanying paper.\n",
    "\n",
    "      Instead of referring to specific labels or points in the graph while asking a question or giving an answer, explain the graph using natural language. Include details like data trends, prominent points, and relationships between different data sets visualized in the graph.\n",
    "\n",
    "      When using the information from the OCR-extracted text, caption and first paragraph to explain the graph, avoid stating that these are your sources. Always answer as if you are directly interpreting the graph according to your AI comprehension, understanding and reasoning. Regarding the format of the given input: the following context described the image:  \"from\": \"human\", \"value\": contains OCR extracted text listed, and 'Figure (integer) caption of the image'.\"from: gpt value: contains the first paragraph in the text that mentioned the figure. Many times the OCR is missing simply because the image does not contain text. \n",
    "\n",
    "\n",
    "Other example may include OCR extracted example like the following: \n",
    "[{'from': 'human',\n",
    "  'value': \"OCR extracted text list, separated by ', ' : 0.8I-R >0.750.70.650.50.60.70.80.91Source variance (y)Mean target risk (RT), 0.8, I-R, >, 0.75, 0.7, 0.65, 0.5, 0.6, 0.7, 0.8, 0.9, 1, Source, variance, (y), Mean, target, risk, (RT) Fig. 6. Mean target risks for the synthetic problem setting, as a function of source variance γ.<image>\\nAnalyze the image in a comprehensive and detailed manner\"},\n",
    " {'from': 'gpt',\n",
    "  'value': 'Weights are estimated by fitting a normal distribution to data from each domain, and computing the ratio of the target probability over the source probability of each source data point ŵ(xi) = p̂T (xi)/p̂S(xi). We compare the importanceweighted risk estimator (R̂ŵ) with its control variate counterpart (R̂β̂). We average their estimated risks over all data sets and specifically over the 10% of data sets with the largest weight variance (indicated with ”>” in the legend of Figures 6, 7 and 8). We also include validation on the labeled target samples (R̂T ) as the oracle solution. After risk estimation, the λ is selected that minimized risk. The classifier is then retrained using all source data and the selected λ, and evaluated using the target risk based on the true target labels as the final measure. This process is repeated for each data set and we report the final average as R̄T .'}]\n",
    "In this case, try to pay attention to the OCR extracted text list and establish its connection to the figure caption as well as the provided paper title and abstract. The OCR provided text is from the figure itself, which you want to help a human understand, so it is important to think about how the those characters relate to the explanation as you write down question and answer pairs. But do not copy and paste the origional OCR text. The OCR text is randomly ordered and convey no explanatory meaning by itself.\n",
    "\n",
    "Can you please think of a couple questions a college-level student may ask regarding the image being discussed, given the context, image caption and the first paragraph that mention the image as well as the entire paper's summary? Think of a couple questions that this college student may ask to first describe the image, and then ask to be walked through the image in a helpuful, textbook-level instruction format. \n",
    "\n",
    "\n",
    "One example is like this: \n",
    "[ { \"from\": \"human\", \"value\": \": How does the length normalization factor affect the BLEU score?\\n<image>\" }, { \"from\": \"gpt\", \"value\": \": The graph shows that the BLEU score is highest when the length normalization factor is set to 0.9. This is true for all values of the source boundary delay D.\" } ]\n",
    "\n",
    "You should think like the following as a example for the input graph: Notice that the caption gives away the x and y axis of the graph. The graph is plotting length normalization factor on the x axis and BLEU on the y  axis because from 'How does the length normalization factor affect the BLEU score?' you can infer that when X affects Y, X is a independent variable and the Y which is BLEU is the dependent variable. You would then notice that the first paragraph explains what the key summary of the graph should be: the author highlights the fact that BLEU is highest at a specific length normalization factor of 0.9. \n",
    "\n",
    "This is the title and summary of the paper: Neural Simultaneous Speech Translation Using Alignment-Based Chunking. In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between latency and quality. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main components: one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the inference conditions. To generate chunked training data, we propose a method that utilizes word alignment while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU absolute.\n",
    "\n",
    "The demostrated response the following have question and answers. Each Question and Answer starts on a new line, and always starts with either 'Question: ' or 'Answer: ' such as the following example. You can have 2-5 question and answer pair. \n",
    "\n",
    "Question: Can you examplify how the BLEU score trends according to the changes in the length normalization factor as depicted on the graph?\n",
    "\n",
    "Answer: On close analysis, we can observe that this graph demonstrates an intriguing correlation between the \"Length Normalization Factor\" and the \"BLEU Score\". It portrays a concave-upward curve, which indicates that as the length normalization factor increases, the BLEU score initially rises, reaching a maximum value, after which it starts to decrease. The maximum value appears when the length normalization factor is set to 0.9, leading to the highest BLEU score, indicating optimum performance of the Neural Machine Translation model at this specific point. The graph interestingly captures this phenomenon, shaping our understanding of how the translation performance varies with different length normalization factors.\n",
    "\n",
    "Question: Based on the graph, what kind of impact does the length normalization factor have on the BLEU score and how can this finding be beneficial for improving translation models like in this study, \"Neural Simultaneous Speech Translation Using Alignment-Based Chunking\"?\n",
    "\n",
    "Answer: As elucidated by the graph, the length normalization factor plays a significant role in fine-tuning the performance of a Neural Machine Translation (NMT) model. Essentially, the optimum translation performance, as measured by the BLEU score, is achieved when the length normalization factor is fixed at 0.9. This signifies that finding the right balance in segment length can lead to better translation results. As for the implications for the 'Alignment-Based Chunking' in the context of our study, understanding this optimum value grants us valuable insight into setting suitable chunk sizes for processing. This consequently could help us strike a balance between latency and quality, improving overall efficiency and effectiveness of our simultaneous machine translation system. Therefore, our finding and its pictorial representation in this graph provide a significant contribution to the neural translation research domain.\n",
    "\n",
    "\n",
    "Other example may include OCR extracted example like the following example. In this case, try to pay attention to the OCR extracted text list and establish its connection to the figure caption as well as the provided paper title and abstract. The OCR provided text is from the figure itself, which you want to help a human understand, so it is important to think about how the those characters relate to the explanation as you write down question and answer pairs. But do not copy and paste the origional OCR text. The OCR text is randomly ordered and convey no explanatory meaning by itself.\n",
    " \n",
    "[{'from': 'human',\n",
    "  'value': \"OCR extracted text list, separated by ', ' : 0.8I-R >0.750.70.650.50.60.70.80.91Source variance (y)Mean target risk (RT), 0.8, I-R, >, 0.75, 0.7, 0.65, 0.5, 0.6, 0.7, 0.8, 0.9, 1, Source, variance, (y), Mean, target, risk, (RT) Fig. 6. Mean target risks for the synthetic problem setting, as a function of source variance γ.<image>\\nAnalyze the image in a comprehensive and detailed manner\"},\n",
    " {'from': 'gpt',\n",
    "  'value': 'Weights are estimated by fitting a normal distribution to data from each domain, and computing the ratio of the target probability over the source probability of each source data point ŵ(xi) = p̂T (xi)/p̂S(xi). We compare the importanceweighted risk estimator (R̂ŵ) with its control variate counterpart (R̂β̂). We average their estimated risks over all data sets and specifically over the 10% of data sets with the largest weight variance (indicated with ”>” in the legend of Figures 6, 7 and 8). We also include validation on the labeled target samples (R̂T ) as the oracle solution. After risk estimation, the λ is selected that minimized risk. The classifier is then retrained using all source data and the selected λ, and evaluated using the target risk based on the true target labels as the final measure. This process is repeated for each data set and we report the final average as R̄T .'}]\n",
    "The demostrated response the following have question and answers\n",
    "\n",
    "Question: How does the graph depict the relationship between the source variance (γ) and the mean target risk (RT) in the context of the synthetic problem setting?\n",
    "\n",
    "Answer: The graph presents a clear depiction of how the mean target risk (RT) changes with varying source variance (γ). It appears to showcase a trend where the mean target risk decreases as the source variance increases, which suggests an inverse relationship between the two variables. This relationship is critical in the context of the synthetic problem setting, as it implies that a higher source variance could potentially lead to a lower mean target risk, thus influencing the overall performance and efficiency of the derivative interpolating subspace frameworks for nonlinear eigenvalue problems.\n",
    "\n",
    "Question: Given the study's focus on derivative interpolating subspace frameworks for nonlinear eigenvalue problems, how does the observed relationship between source variance and mean target risk in the graph contribute to the overall findings of the research?\n",
    "\n",
    "Answer: The observed relationship between source variance and mean target risk in the graph is instrumental in understanding the performance of the proposed subspace framework. The graph indicates that as the source variance increases, the mean target risk decreases. This could suggest that the framework performs better when dealing with higher source variance. This insight is particularly significant as it provides a valuable parameter (source variance) that can be adjusted to optimize the performance of the derivative interpolating subspace frameworks for nonlinear eigenvalue problems. Therefore, this graph not only substantiates the research's findings but also offers a practical guideline for enhancing the efficiency of the proposed framework.\n",
    "\n",
    "Question: Based on the graph and the context of the paper, could you explain how the concept of source variance impacts the performance of the derivative interpolating subspace framework for nonlinear eigenvalue problems?\n",
    "\n",
    "Answer: The graph provides a visual representation of the relationship between source variance and mean target risk, two critical parameters in the context of the derivative interpolating subspace framework for nonlinear eigenvalue problems. As source variance increases, the graph shows a corresponding decrease in mean target risk. This suggests that the derivative interpolating subspace framework performs more effectively when dealing with higher source variance. In the context of nonlinear eigenvalue problems, this could mean that the framework is particularly adept at handling complex problems with a high degree of variance. This insight is significant as it not only validates the effectiveness of the proposed framework but also highlights its potential for tackling complex, high-variance nonlinear eigenvalue problems.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "system_message_token_count = num_tokens_from_messages([system_message,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88550310-0b66-4d70-8938-3ad8b243de18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70404"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_twenty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28a8a4a9-c1fc-439d-bf45-a002386e8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token count 249334421\n",
      "output tokens 15492050\n"
     ]
    }
   ],
   "source": [
    "total_input_tokens = input_tokens + system_message_token_count * len(first_twenty)\n",
    "\n",
    "total_output = output_tokens\n",
    "\n",
    "print('input token count', total_input_tokens)\n",
    "\n",
    "print('output tokens', total_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f42f068-0a34-4921-b714-79166a3efbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gpt_cost(input_token_count, output_token_count, model = 'gpt-4'):\n",
    "    if model == 'gpt-4':\n",
    "        return 0.03*(input_token_count//1000) + 0.006*(output_token_count//1000)\n",
    "    elif model == 'gpt-3.5-turbo':\n",
    "        return 0.0015*(input_token_count//1000) + 0.002*(output_token_count//1000)\n",
    "    else:\n",
    "        return 0.0005*(input_token_count + output_token_count) // 1000\n",
    "\n",
    "calculate_gpt_cost(total_input_tokens, total_output, model='palm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "047745f1-7adf-44d0-b345-5a05f33f29fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220.04502585080394, 774, 3541)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_output_token = (total_output) / len(first_twenty)\n",
    "mean_input = input_tokens // len(first_twenty)\n",
    "mean_input_with_system_message = mean_input + system_message_token_count\n",
    "mean_output_token, mean_input, mean_input_with_system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17b62f93-02a5-4fc3-90b5-f35124ea5e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55bb46e90d3412b9601907e9cb676a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/liuhaotian--LLaVA-Instruct-150K to /home/jupyter/.cache/huggingface/datasets/liuhaotian___json/liuhaotian--LLaVA-Instruct-150K-4c8b43235748f679/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3885c1f354421ca0685fca3ffab27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad893c716fa4e36839da071cbd2669f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/79.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24496f63c78e4e39b842a475be44707a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/126M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf748fe1b41d42adb1948e5cc05c5695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a0130bf05547039ed9d76769b78317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/229M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51fafb597a84b1fa19924812be4e4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/131M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d999409e8748c7ba47de161cca0236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/liuhaotian___json/liuhaotian--LLaVA-Instruct-150K-4c8b43235748f679/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "compare_to_llava = load_dataset('liuhaotian/LLaVA-Instruct-150K', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30d55132-969b-4597-92cc-6d33747ed6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_total_output_tokens = num_tokens_from_messages(compare_to_llava['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c6b72e8-fe88-4b7b-b4ab-73ae858b71f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total llava output tokens 109185835\n",
      "average llava output tokens 276\n"
     ]
    }
   ],
   "source": [
    "average_llava_token_length = llava_total_output_tokens // len(compare_to_llava['conversations'])\n",
    "\n",
    "print('total llava output tokens', llava_total_output_tokens)\n",
    "print('average llava output tokens', average_llava_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "376ea05a-b302-4011-bcd0-fe5c7a18db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jupyter/.cache/huggingface/datasets/alexshengzhili___parquet/alexshengzhili--SciCapInstructed410K-f89dbc60ba315149/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total previous output tokens 31959233\n",
      "average previous output tokens 90\n"
     ]
    }
   ],
   "source": [
    "prev_single_turn = load_dataset('alexshengzhili/SciCapInstructed410K', split='train')\n",
    "prev_single_turn_total = num_tokens_from_messages(prev_single_turn['response'])\n",
    "\n",
    "prev_single_turn_average = prev_single_turn_total // len(prev_single_turn['response'])\n",
    "\n",
    "print('total previous output tokens', prev_single_turn_total)\n",
    "print('average previous output tokens', prev_single_turn_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eeb3831c-5bc6-4ae4-818f-3d3d870dac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: The graph shows the performance of still-to-video FR systems versus the number of synthetic ROIs generated under both pose and lighting effects. What is the relationship between the number of synthetic ROIs and the performance of the FR system?\\n\\nAnswer: The graph shows that the performance of the FR system improves as the number of synthetic ROIs increases. This is because the more synthetic ROIs that are used to train the classifier, the more likely it is that the classifier will be able to generalize to new data.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_single_turn['response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64c8825b-1cba-4ad1-8ce6-6c90ffad3477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29270493741243997"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_single_turn_total / llava_total_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8aaa78e4-90d4-41fb-8e93-6b614b94df70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7094349738681762"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_output*5 / llava_total_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01ddb3-ae1b-4a34-8a30-0ec4f48de9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
